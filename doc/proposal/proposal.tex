%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Multi-Label Classification} % The article title

\author{
	\authorstyle{Willie Maddox} % Authors
%	\authorstyle{Willie Maddox\textsuperscript{1,2,3}} % Authors
%	\newline\newline % Space before institutions
%	\textsuperscript{2}\institution{University of Texas at Austin, Texas, United States of America}\\ % Institution 2
%	\textsuperscript{3}\institution{\texttt{LaTeXTemplates.com}} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This document serves as the proposal for the final Capstone project for the Machine Learning Engineer Nanodegree offered through Udacity. For this work we will train a model to recognize multiple items in an image. However, unlike traditional classification problems, we will train our models using image datasets with multiple labels per image.  Our goal is to achieve as good as if not better results than popular single-label classification models.}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Domain Background}\label{sec:1} % 1 - 2 para

% In this section, provide brief details on the background information of the domain from which the project is proposed. Historical information relevant to the project should be included. It should be clear how or why a problem in the domain can or should be solved. Related academic research should be appropriately cited in this section, including why that research is relevant.  Additionally, a discussion of your personal motivation for investigating a particular problem in the domain is encouraged but not required.

In machine learning, image classification is the problem of deciding to which category a particular image belongs. Much of the previous work for solving image classification problems has been focused on using a single label per image to train a classifier.  For some of the more popular datasets such as ImageNet~\cite{ILSVRC15} and CIFAR-10~\cite{Krizhevsky2009}, each image has associated with it a single label and there are many images per label. This works well for a dataset like MNIST~\cite{mnist} where each instance is a black and white image of a single handwritten digit  between 0 and 9.  But for images that illustrate the real world such as photographs, there is almost never a single contextual topic in the image. For example, Fig.\ref{screw-1924174_640} is a picture of bolts, but there are also nuts, washers and a wooden table.  So in reality this image has (at least) four tags.  The goal of this project is to create a classifier that can be trained using images that have multiple labels per image.

\begin{figure}
	\includegraphics[width=\linewidth]{screw-1924174_640.jpg} % Figure image
	\caption{Nuts n Bolts} % Figure caption
	\label{screw-1924174_640} % Label for referencing with \ref{bear}
\end{figure}

\section{Problem Statement} % 1 para

%In this section, clearly describe the problem that is to be solved. The problem described should be well defined and should have at least one relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that:
%\begin{description}
%	\item[Quantifiable] The problem can be expressed in mathematical or logical terms.
%	\item[Measurable] The problem can be measured by some metric and clearly observed.
%	\item[Replicable] The problem can be reproduced and occurs more than once. 
%\end{description}

The problem that we will try to solve is finding multiple features (or labels) in a single image. We would also like to know how much better our trained multi-label classifier is at predicting the top $k$ labels for a given image as compared to a standard single-label classifiers like VGG16~\cite{SimonyanZ14a} trained on ImageNet?

\section{Datasets and Inputs}\label{sec:3} % 2 - 3 para

%In this section, the dataset(s) and/or input(s) being considered for the project should be thoroughly described, such as how they relate to the problem and why they should be used. Information such as how the dataset or input is (was) obtained, and the characteristics of the dataset or input, should be included with relevant references and citations as necessary It should be clear how the dataset(s) or input(s) will be used in the project and whether their use is appropriate given the context of the problem.

For this study we will use the NUS-WIDE\citep{nus-wide-civr09} dataset as our primary dataset. We may consider using the MS-COCO\citep{MSCOCO} dataset as well.  
%How do the dataset(s) and/or input(s) relate to the problem?
These datasets contain images with multiple labels per image making them ideal for this kind of study.
%Why should the dataset(s) and/or input(s) be used?
They are also large enough that we can use them for training.
%How is (was) the dataset or input obtained?
Both datasets are feely downloadable from the internet.  However, we had to explicitly request permission to use and download the image set for the NUS-WIDE dataset.  The authors were happy to grant us permission.

%What are the characteristics of the dataset or input?
%How will the dataset(s) or input(s) be used in the project?
The NUS-WIDE dataset contains 269,648 images and 81 labels with 2.4 labels per image on average.  The images vary in size (height and width values fall within the range between about 150 pixels to 250 pixels) and shape (portrait and landscape) and all have 3 color channels. We will use the original train/test split of 161,789 and 107,859. The MS-COCO dataset contains a training set of 82,783 images and a test set of 40,504 images.  However, the test set does not come with ground truth so we will use a subset of the training set for validation and testing. The MS-COCO dataset has 80 labels with 2.9 labels per image on average.  
%Is it appropriate to use these datasets given the context of the problem?
Since we will be fine-tuning a model pretrained on ImageNet, these two datasets are good candidates since all their labels are a subset of the original 1000 ImageNet labels.

\section{Solution Statement}\label{sec:4} % 1 para

%In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Additionally, describe the solution thoroughly such that it is clear that the solution is 

%\begin{description}
%	\item[Quantifiable] The solution can be expressed in mathematical or logical terms.
%	\item[Measurable] The solution can be measured by some metric and clearly observed.
%	\item[Replicable] The solution can be reproduced and occurs more than once.
%\end{description}

The solution is to train a model that is capable of generating (or recommending) a finite set of labels based soley on an input image.  For the training we will explore a few algorithms.  We will compare between different back propogation optimizers such as rmsprop, Adam and SGD.  We will use transfer learning to fine-tune a pre-existing model based on the labels from our datasets.  We will measure the performance of our model by comparing the top 3 predicted labels from our model against the known ground truth labels for each of our images\footnote{since both datasets have $\approx$ 3 labels per image on average, $k = 3$ seems appropriate}.  The specific metrics used are discussed in the following sections. We set aside a subset of our training data which will be used exclusively for testing. We will use the Keras python package (with the TensorFlow backend) to create our neural nets and we will train them using a single Nvidia 1080 Ti GPU.

\section{Benchmark Model\label{sec:5}} % 1 - 2 para

% In this section, provide the details for a benchmark model or result that relates to the domain, problem statement, and intended solution. Ideally, the benchmark model or result contextualizes existing methods or known information in the domain and problem given, which could then be objectively compared to the solution. Describe how the benchmark model or result is measurable (can be measured by some metric and clearly observed) with thorough detail.

To benchmark the solution above, we will compare the testing set against the original VGG16 model pretrained on ImageNet\footnote{\url{https://keras.io/applications/}} (Just the vanilla VGG16 model with 1000 classes. No fine tuning.) This will be our primary benchmark model. Since we will be using this same base model for transfer learning, we should expect our model to perform better at classifing single-label images from the 81 category NUS-WIDE label set.

For the second model, we will use Clarifai's image recognition API\footnote{\url{https://www.clarifai.com/}}.  Clarifai’s image recognition systems recognize various categories, objects, and tags in images, as well as find similar images. The company’s image recognition systems also allow its users to find similar images in large uncategorized repositories using a combination of semantic and visual similarities.  

We will use the evaluation metrics below to quantify how well our model does against state-of-the-art models. These models include KNN~\cite{nus-wide-civr09}, WARP~\cite{GongJLTI13}, CNN-RNN~\cite{WangYMHHX16}, and ResNet-SRN~\cite{ZhuLOYW17}.

\section{Evaluation Metrics\label{sec:6}} % 1 - 2 para

% In this section, propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).

After reading through previous literature on multi-label classification, we found that there are quite a few metrics appropriate for this problem. The mean average precision (mAP) is a widely used metric for comparing between trained models and has been regarded as the best metric for classification problems~\cite{Lavrenko_2014}.  Other popular metrics include precision, recall, $F_1$ score, Jaccard index, 0/1 loss and Hamming loss~\cite{Tsoumakas:2007,SOKOLOVA2009427,Herrera:2016}. At this point, though, it is unclear which of these metrics will provide the best insight for our results.

%\begin{equation}
%\mathrm{IOU} = \frac{1}{N}\sum_{i=1}^{N}\frac{\vert y^i \wedge \hat{y} \vert}{\vert y^i \vee \hat{y} \vert}, \label{eq:IOU}
%\end{equation}

\section{Project Design\label{sec:7}} % 1 page

% In this final section, summarize a theoretical workflow for approaching a solution given the problem. Provide thorough discussion for what strategies you may consider employing, what analysis of the data might be required before being used, or which algorithms will be considered for your implementation. The workflow and discussion that you provide should align with the qualities of the previous sections. Additionally, you are encouraged to include small visualizations, pseudocode, or diagrams to aid in describing the project design, but it is not required. The discussion should clearly outline your intended workflow of the capstone project.

The tenative schedule for solving the given problem will go as follows. First we will read the datasets and split them into training/validation/testing subsets.  Each image will need to be resized and pixel values scaled to a range between 0 and 1. We will either create one model out of both datasets or two models, one for each dataset.

Next we will set up our neural network. Since training is expensive we will implement transfer learning to leverage the pretrained weights of a preexisting model.  We will use the VGG16 model pretrained on ImageNet.  This model is provided to us through the Keras codebase.  When we load the model, we will leave off the 3 fully-connected (fc) layers at the top of the network.  This will serve as our base model for the entire project.  We will run the base model on each input (\textit{i.e.} image) in our dataset and create a set of bottleneck features from the resulting predictions.

We will then use these bottleneck features to train a small fc model which will later be attached to our base model.  The number of layers and the number of neurons per layer will be hyperparameters that we will explore, but we will start out with a simple fc network as shown in Table~\ref{tab:top_model}.
\begin{table}
\centering
\caption{Initial configuration of our proposed top neural network model. The fc's are the fully-connected layers and the ReLU is a rectified linear unit activation layer. In the output shape, "None" is just a placeholder for the batch size.  For a model trained on the  Pascal VOC data, the final 2 layers would become (None, 80)}
\label{tab:top_model}
\begin{tabular}{ll}
%\toprule
\hline
Layer Type & Output Shape \\
%\midrule
\hline
fc      & (None, 1024) \\
ReLU    & (None, 1024) \\
dropout & (None, 1024) \\
fc      & (None, 81)   \\
sigmoid & (None, 81)   \\
%\bottomrule
\hline
\end{tabular}
\end{table}
For multi-\textit{class} classification problems it is common to use a softmax layer for the final output layer. This forces the loss function to place bias on a single label during the backpropogation phase.  Since our problem is a multi-\textit{label} classification problem, we do not want to give a preference to any particular (ground truth) label since we are treating them equally.  For this reason, we will use a standard sigmoid layer is our final output classifier.  For the loss function we will use binary cross entropy and for backpropogation we will use rmsprop to update the weights.  After training this small fc model, we will attach it to the top of the base model and this will serve as our main model. 

Now that we have our main model we can fine-tune the entire network.  For this, we will again use binary cross entropy for the loss function, but since we're fine-tuning, we want to use a slow learning rate so that we do not overfit.  For this we will use stochastic gradient descent with a 1e-4 learning rate and 0.9 momentum. We will also freeze most of the bottom layers of the base model so that training can go faster.  The number of layers at the end of the base layer that we will leave unfrozen will be a hyperparameter for us to explore.

Training this final model will be significantly slower than training the bottleneck features, especially if we decide to unfreeze some of the top convolutional layers from the base model.  This is because file input/output between CPU and GPU is a computational bottleneck.  To help speed things up, we will process images from our training set in batches.  The size of each batch is also a hyperparameter, but will most likely be 32, 64, 128, or some other power of 2. For each batch, we will augment the images using a number of random transformations (i.e. zoom, rotation, horizontal flip, noise, etc.).  This will ensure that our model never sees the same image twice. After each epoch, we will calculate the loss and accuracy of the model using the validation set.  This will allow us to detect if our model is overfitting.  In addition, we will check if the validation loss is less than the validation loss from the previous epoch and if so, we will create a checkpoint by saving the weights to a file.

Once the model is trained we will run predictions on our holdout test set using the metrics listed in Section~\ref{sec:6} and compare them to those currently found in the literature.  We will also compare our results to the benchmark models described in Section~\ref{sec:5}.

%Too compensate for class imbalance, We will use stratification to construct the $k-fold$ cross validation subsets\citep{Kohavi_1995_1137, Sechidis2011}.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliography{../example}
%\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
